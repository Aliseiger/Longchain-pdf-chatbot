import streamlit as st
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama
from langchain_classic.chains import ConversationalRetrievalChain
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from PyPDF2 import PdfReader

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØµÙØ­Ù‡
st.set_page_config(page_title="Chat with PDF (Ollama + FAISS)", layout="wide")
st.title("ğŸ“„ Chat with your PDF (Offline + Local AI)")

# Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¢Ù¾Ù„ÙˆØ¯ PDF
uploaded_file = st.file_uploader("Upload a PDF file:", type=["pdf"])

if uploaded_file:
    with st.spinner("Reading your PDF..."):
        pdf_reader = PdfReader(uploaded_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() or ""

    # ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ ØªÚ©Ù‡â€ŒÙ‡Ø§
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)
    docs = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]

    # Ù…Ø±Ø­Ù„Ù‡ Û²: Ø³Ø§Ø®Øª embedding
    with st.spinner("Creating embeddings with Ollama... (first time may take a minute)"):
        embeddings = OllamaEmbeddings(
        model="nomic-embed-text",
        base_url="http://localhost:11434")  # Ù…Ø·Ù…Ø¦Ù† Ø´Ùˆ Ollama Ù‡Ù…ÛŒÙ† Ù¾ÙˆØ±Øª Ø±Ùˆ Ø¯Ø§Ø±Ù‡
                    
        vectorstore = FAISS.from_documents(docs, embeddings)
    
          # ÛŒØ§ llama3 Ø¨Ø±Ø§ÛŒ ØªØ³Øª

    # Ù…Ø±Ø­Ù„Ù‡ Û³: Ø³Ø§Ø®Øª Ù…Ø¯Ù„ Ú†Øª
    llm = ChatOllama(model="llama3")
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )

    # Ø­Ø§ÙØ¸Ù‡ Ú†Øª
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    st.subheader("ğŸ’¬ Ask questions about your PDF")
    user_input = st.text_input("Type your question:")

    if user_input:
        with st.spinner("Thinking..."):
            result = qa_chain({
                "question": user_input,
                "chat_history": st.session_state.chat_history
            })
            answer = result["answer"]
            st.session_state.chat_history.append((user_input, answer))

            st.markdown(f"**ğŸ§  You:** {user_input}")
            st.markdown(f"**ğŸ¤– Bot:** {answer}")

    # Ù†Ù…Ø§ÛŒØ´ ØªØ§Ø±ÛŒØ®Ú†Ù‡
    if st.session_state.chat_history:
        st.markdown("---")
        st.markdown("### Chat History")
        for q, a in reversed(st.session_state.chat_history):
            st.markdown(f"**You:** {q}")
            st.markdown(f"**Bot:** {a}")
